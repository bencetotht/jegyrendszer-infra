apiVersion: helm.toolkit.fluxcd.io/v2beta1
kind: HelmRelease
metadata:
  name: prometheus-release
  namespace: monitoring
spec:
  interval: 5m
  chart:
    spec:
      chart: prometheus
      version: '25.11.0'
      sourceRef:
        kind: HelmRepository
        name: prometheus-repo
        namespace: flux-system
      interval: 1m
  values:
    alertmanagers:
      - config:
          receivers:
          - name: slack-receiver
            slack_configs:
            - api_url: ""
              channel: "#"
              send_resolved: true
              text: >-
                  {{ range .Alerts -}}
                  *Alert:* {{ .Annotations.title }}{{ if .Labels.severity }} - `{{ .Labels.severity }}`{{ end }}
                  *Description:* {{ .Annotations.description }}
                  *Details:*
                    {{ range .Labels.SortedPairs }} â€¢ *{{ .Name }}:* `{{ .Value }}`
                    {{ end }}
                  {{ end }}
    serverFiles:
      alerting_rules.yml:
        groups:
        - name: node_exporter_alerts
          rules:
          - alert: NodeDown
            expr: up == 0
            for: 2m
            labels:
              severity: warning
            annotations:
              title: Node {{ $labels.instance }} is down
              description: Failed to scrape {{ $labels.job }} on {{ $labels.instance }} for more than 2 minutes. Node seems down.
          - alert: HighCPUUsage
            expr: ((sum by(instance) (irate(node_cpu_seconds_total{job="kubernetes-service-endpoints", mode!="idle"}[2m])) / on (instance) group_left sum by (instance) (irate(node_cpu_seconds_total{job="kubernetes-service-endpoints"}[2m]))) * 100) > 80
            for: 2m
            labels:
              severity: warning
            annotations:
              summary: Host high CPU load (instance {{ $labels.instance }})
              description: CPU load is > 80%\n  VALUE = {{ $value }}
          - alert: HostHighCpuLoad
            expr: 100 - (avg by(instance) (rate(node_cpu_seconds_total{mode="idle"}[2m])) * 100) > 80
            for: 0m
            labels:
              severity: warning
            annotations:
              summary: Host high CPU load (instance {{ $labels.instance }})
              description: CPU load is > 80%\n  VALUE = {{ $value }}
          - alert: HostOutOfMemory
            expr: sum by (node) (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes ) * 100 < 10
            for: 2m
            labels:
              severity: warning
            annotations:
              summary: Host out of memory (instance {{ $labels.instance }})
              description: Node memory is filling up (< 10% left)\n  VALUE = {{ $value }}
          - alert: HostMemoryUnderMemoryPressure
            expr: rate(node_vmstat_pgmajfault[1m]) > 1000
            for: 2m
            labels:
              severity: warning
            annotations:
              summary: Host memory under memory pressure (instance {{ $labels.instance }})
              description: The node is under heavy memory pressure. High rate of major page faults\n  VALUE = {{ $value }}
          - alert: HostUnusalNetworkThroughputIn
            expr: (sum by (node) (rate(node_network_receive_bytes_total[10m])) / 1024 / 1024) > 50
            for: 10m
            labels:
              severity: warning
            annotations:
              summary: Host unusual network throughput in (instance {{ $labels.instance }})
              description: Host network interfaces are probably receiving too much data (> 100 MB/s)\n  VALUE = {{ $value }}
          - alert: HostUnusualNetworkThroughputOut
            expr: (sum by (node) (rate(node_network_transmit_bytes_total[10m])) / 1024 / 1024) > 50
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: Host unusual network throughput out (instance {{ $labels.instance }})
              description: Host network interfaces are probably sending too much data (> 100 MB/s)\n  VALUE = {{ $value }}
          - alert: HostUnusualDiskReadLatency
            expr: rate(node_disk_read_time_seconds_total[2m]) / rate(node_disk_reads_completed_total[2m]) > 0.1 and rate(node_disk_reads_completed_total[2m]) > 0
            for: 2m
            labels:
              severity: warning
            annotations:
              summary: Host unusual disk read latency (instance {{ $labels.instance }})
              description: Disk latency is growing (read operations > 100ms)\n  VALUE = {{ $value }}
          - alert: HostUnusualDiskWriteLatency
            expr: rate(node_disk_write_time_seconds_totali{device!~"mmcblk.+"}[1m]) / rate(node_disk_writes_completed_total{device!~"mmcblk.+"}[1m]) > 0.1 and rate(node_disk_writes_completed_total{device!~"mmcblk.+"}[1m]) > 0
            for: 2m
            labels:
              severity: warning
            annotations:
              summary: Host unusual disk write latency (instance {{ $labels.instance }})
              description: Disk latency is growing (write operations > 100ms)\n  VALUE = {{ $value }}
          - alert: HostOomKillDetected
            expr: increase(node_vmstat_oom_kill[1m]) > 0
            for: 0m
            labels:
              severity: warning
            annotations:
              summary: Host OOM kill detected (instance {{ $labels.instance }})
              description: OOM kill detected\n  VALUE = {{ $value }}
          - alert: HostNetworkReceiveErrors
            expr: rate(node_network_receive_errs_total[2m]) / rate(node_network_receive_packets_total[2m]) > 0.01
            for: 2m
            labels:
              severity: warning
            annotations:
              summary: Host Network Receive Errors (instance {{ $labels.instance }}:{{ $labels.device }})
              description: Instance interface has encountered {{ printf "%.0f" $value }} receive errors in the last five minutes.\n  VALUE = {{ $value }}
          - alert: HostNetworkTransmitErrors
            expr: rate(node_network_transmit_errs_total[2m]) / rate(node_network_transmit_packets_total[2m]) > 0.01
            for: 2m
            labels:
              severity: warning
            annotations:
              summary: Host Network Transmit Errors (instance {{ $labels.instance }}:{{ $labels.device }})
              description: Instance has encountered {{ printf "%.0f" $value }} transmit errors in the last five minutes.\n  VALUE = {{ $value }}
          - alert: HostClockSkew
            expr: (node_timex_offset_seconds > 0.05 and deriv(node_timex_offset_seconds[5m]) >= 0) or (node_timex_offset_seconds < -0.05 and deriv(node_timex_offset_seconds[5m]) <= 0)
            for: 2m
            labels:
              severity: warning
            annotations:
              summary: Host clock skew (instance {{ $labels.instance }})
              description: Clock skew detected. Clock is out of sync.\n  VALUE = {{ $value }}
          - alert: HostClockNotSynchronising
            expr: min_over_time(node_timex_sync_status[1m]) == 0 and node_timex_maxerror_seconds >= 16
            for: 2m
            labels:
              severity: warning
            annotations:
              summary: Host clock not synchronising (instance {{ $labels.instance }})
              description: Clock not synchronising.\n  VALUE = {{ $value }}
        - name: kubernetes_alerts
          rules:
          - alert: KubernetesNodeNotReady
            expr: kube_node_status_condition{condition="Ready",status="true"} == 0
            for: 10m
            labels:
              severity: critical
            annotations:
              summary: Kubernetes Node not ready (instance {{ $labels.instance }})
              description: "Node {{ $labels.node }} has been unready for a long time\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
          - alert: KubernetesNodeMemoryPressure
            expr: kube_node_status_condition{condition="MemoryPressure",status="true"} == 1
            for: 2m
            labels:
              severity: critical
            annotations:
              summary: Kubernetes Node memory pressure (instance {{ $labels.instance }})
              description: "Node {{ $labels.node }} has MemoryPressure condition\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
          - alert: KubernetesNodeDiskPressure
            expr: kube_node_status_condition{condition="DiskPressure",status="true"} == 1
            for: 2m
            labels:
              severity: critical
            annotations:
              summary: Kubernetes Node disk pressure (instance {{ $labels.instance }})
              description: "Node {{ $labels.node }} has DiskPressure condition\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
          - alert: KubernetesNodeOutOfPodCapacity
            expr: sum by (node) ((kube_pod_status_phase{phase="Running"} == 1) + on(uid) group_left(node) (0 * kube_pod_info{pod_template_hash=""})) / sum by (node) (kube_node_status_allocatable{resource="pods"}) * 100 > 90
            for: 2m
            labels:
              severity: warning
            annotations:
              summary: Kubernetes Node out of pod capacity (instance {{ $labels.instance }})
              description: "Node {{ $labels.node }} is out of pod capacity\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
          - alert: KubernetesContainerOomKiller
            expr: (kube_pod_container_status_restarts_total - kube_pod_container_status_restarts_total offset 10m >= 1) and ignoring (reason) min_over_time(kube_pod_container_status_last_terminated_reason{reason="OOMKilled"}[10m]) == 1
            for: 0m
            labels:
              severity: warning
            annotations:
              summary: Kubernetes Container oom killer (instance {{ $labels.instance }})
              description: "Container {{ $labels.container }} in pod {{ $labels.namespace }}/{{ $labels.pod }} has been OOMKilled {{ $value }} times in the last 10 minutes.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
          - alert: KubernetesJobFailed
            expr: kube_job_status_failed > 0
            for: 0m
            labels:
              severity: warning
            annotations:
              summary: Kubernetes Job failed (instance {{ $labels.instance }})
              description: "Job {{ $labels.namespace }}/{{ $labels.job_name }} failed to complete\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
          - alert: KubernetesVolumeOutOfDiskSpace
            expr: kubelet_volume_stats_available_bytes / kubelet_volume_stats_capacity_bytes * 100 < 10
            for: 2m
            labels:
              severity: warning
            annotations:
              summary: Kubernetes Volume out of disk space (instance {{ $labels.instance }})
              description: "Volume is almost full (< 10% left)\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
          - alert: KubernetesVolumeOutOfDiskSpace
            expr: kubelet_volume_stats_available_bytes / kubelet_volume_stats_capacity_bytes * 100 < 10
            for: 2m
            labels:
              severity: warning
            annotations:
              summary: Kubernetes Volume out of disk space (instance {{ $labels.instance }})
              description: "Volume is almost full (< 10% left)\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
          - alert: KubernetesVolumeFullInFourDays
            expr: predict_linear(kubelet_volume_stats_available_bytes[6h:5m], 4 * 24 * 3600) < 0
            for: 0m
            labels:
              severity: critical
            annotations:
              summary: Kubernetes Volume full in four days (instance {{ $labels.instance }})
              description: "Volume under {{ $labels.namespace }}/{{ $labels.persistentvolumeclaim }} is expected to fill up within four days. Currently {{ $value | humanize }}% is available.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
          - alert: KubernetesPersistentvolumeError
            expr: kube_persistentvolume_status_phase{phase=~"Failed|Pending", job="kube-state-metrics"} > 0
            for: 0m
            labels:
              severity: critical
            annotations:
              summary: Kubernetes PersistentVolume error (instance {{ $labels.instance }})
              description: "Persistent volume {{ $labels.persistentvolume }} is in bad state\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
          - alert: KubernetesStatefulsetDown
            expr: kube_statefulset_replicas != kube_statefulset_status_replicas_ready > 0
            for: 1m
            labels:
              severity: critical
            annotations:
              summary: Kubernetes StatefulSet down (instance {{ $labels.instance }})
              description: "StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }} went down\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
          - alert: KubernetesHpaScaleInability
            expr: kube_horizontalpodautoscaler_status_condition{status="false", condition="AbleToScale"} == 1
            for: 2m
            labels:
              severity: warning
            annotations:
              summary: Kubernetes HPA scale inability (instance {{ $labels.instance }})
              description: "HPA {{ $labels.namespace }}/{{ $labels.horizontalpodautoscaler }} is unable to scale\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
          - alert: KubernetesHpaScaleMaximum
            expr: kube_horizontalpodautoscaler_status_desired_replicas >= kube_horizontalpodautoscaler_spec_max_replicas
            for: 2m
            labels:
              severity: info
            annotations:
              summary: Kubernetes HPA scale maximum (instance {{ $labels.instance }})
              description: "HPA {{ $labels.namespace }}/{{ $labels.horizontalpodautoscaler }} has hit maximum number of desired pods\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
          - alert: KubernetesPodCrashLooping
            expr: increase(kube_pod_container_status_restarts_total[1m]) > 3
            for: 2m
            labels:
              severity: warning
            annotations:
              summary: Kubernetes pod crash looping (instance {{ $labels.instance }})
              description: "Pod {{ $labels.namespace }}/{{ $labels.pod }} is crash looping\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
        - name: traefik_alerts
          rules:
          - alert: TooManyRequests
            expr: avg(traefik_service_open_connections) > 50
            for: 1m
            labels:
              severity: critical
          - alert: TraefikHighHttp4xxErrorRateService
            expr: sum(rate(traefik_service_requests_total{code=~"4.*"}[3m])) by (service) / sum(rate(traefik_service_requests_total[3m])) by (service) * 100 > 50
            for: 1m
            labels:
              severity: critical
            annotations:
              summary: Traefik high HTTP 4xx error rate service (instance {{ $labels.instance }})
              description: "Traefik service 4xx error rate is above 5%\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
          - alert: TraefikHighHttp5xxErrorRateService
            expr: sum(rate(traefik_service_requests_total{code=~"5.*"}[3m])) by (service) / sum(rate(traefik_service_requests_total[3m])) by (service) * 100 > 5
            for: 1m
            labels:
              severity: critical
            annotations:
              summary: Traefik high HTTP 5xx error rate service (instance {{ $labels.instance }})
              description: "Traefik service 5xx error rate is above 5%\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
        - name: mongodb_alerts
          rules:
          - alert: MongodbDown
            expr: mongodb_up == 0
            for: 0m
            labels:
              severity: critical
            annotations:
              summary: MongoDB Down (instance {{ $labels.instance }})
              description: "MongoDB instance is down\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
          - alert: MongodbNumberCursorsOpen
            expr: mongodb_ss_metrics_cursor_open{csr_type="total"} > 10 * 1000
            for: 2m
            labels:
              severity: warning
            annotations:
              summary: MongoDB number cursors open (instance {{ $labels.instance }})
              description: "Too many cursors opened by MongoDB for clients (> 10k)\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
          - alert: MongodbCursorsTimeouts
            expr: increase(mongodb_ss_metrics_cursor_timedOut[1m]) > 100
            for: 2m
            labels:
              severity: warning
            annotations:
              summary: MongoDB cursors timeouts (instance {{ $labels.instance }})
              description: "Too many cursors are timing out\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
          - alert: MongodbTooManyConnections
            expr: avg by(instance) (rate(mongodb_ss_connections{conn_type="current"}[1m])) / avg by(instance) (sum (mongodb_ss_connections) by (instance)) * 100 > 80
            for: 2m
            labels:
              severity: warning
            annotations:
              summary: MongoDB too many connections (instance {{ $labels.instance }})
              description: "Too many connections (> 80%)\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
        - name: redis_alerts
          rules:
          - alert: RedisDown
            expr: redis_up == 0
            for: 0m
            labels:
              severity: critical
            annotations:
              summary: Redis down (instance {{ $labels.instance }})
              description: "Redis instance is down\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
          - alert: RedisMissingMaster
            expr: (count(redis_instance_info{role="master"}) or vector(0)) < 1
            for: 0m
            labels:
              severity: critical
            annotations:
              summary: Redis missing master (instance {{ $labels.instance }})
              description: "Redis cluster has no node marked as master.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
          - alert: RedisTooManyMasters
            expr: count(redis_instance_info{role="master"}) > 1
            for: 0m
            labels:
              severity: critical
            annotations:
              summary: Redis too many masters (instance {{ $labels.instance }})
              description: "Redis cluster has too many nodes marked as master.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
          - alert: RedisDisconnectedSlaves
            expr: count without (instance, job) (redis_connected_slaves) - sum without (instance, job) (redis_connected_slaves) - 1 > 0
            for: 0m
            labels:
              severity: critical
            annotations:
              summary: Redis disconnected slaves (instance {{ $labels.instance }})
              description: "Redis not replicating for all slaves. Consider reviewing the redis replication status.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
          - alert: RedisReplicationBroken
            expr: delta(redis_connected_slaves[1m]) < 0
            for: 0m
            labels:
              severity: critical
            annotations:
              summary: Redis replication broken (instance {{ $labels.instance }})
              description: "Redis instance lost a slave\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
          - alert: RedisClusterFlapping
            expr: changes(redis_connected_slaves[1m]) > 1
            for: 2m
            labels:
              severity: critical
            annotations:
              summary: Redis cluster flapping (instance {{ $labels.instance }})
              description: "Changes have been detected in Redis replica connection. This can occur when replica nodes lose connection to the master and reconnect (a.k.a flapping).\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
          - alert: RedisTooManyConnections
            expr: redis_connected_clients / redis_config_maxclients * 100 > 90
            for: 2m
            labels:
              severity: warning
            annotations:
              summary: Redis too many connections (instance {{ $labels.instance }})
              description: "Redis is running out of connections (> 90% used)\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
          - alert: RedisRejectedConnections
            expr: increase(redis_rejected_connections_total[1m]) > 0
            for: 0m
            labels:
              severity: critical
            annotations:
              summary: Redis rejected connections (instance {{ $labels.instance }})
              description: "Some connections to Redis has been rejected\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
